{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATIONS#\n",
    "\n",
    "# core importations #\n",
    "import caveclient\n",
    "from caveclient import CAVEclient\n",
    "import cloudvolume\n",
    "from cloudvolume import CloudVolume\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dash importations #\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output, State, dash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030787f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS #\n",
    "\n",
    "# defines function for converting nanometer coordinates to 4x4x40 resolution #\n",
    "def coordConvert(coords):\n",
    "    x = coords\n",
    "    x[0] /= 4\n",
    "    x[1] /= 4\n",
    "    x[2] /= 40\n",
    "    x[0] = str(x[0])\n",
    "    x[1] = str(x[1])\n",
    "    x[2] = str(x[2])\n",
    "    return x\n",
    "\n",
    "# defines function to convert list of coordinates in [4,4,40] resolution to root id #\n",
    "def coordsToRoot(coords):\n",
    "\n",
    "    # sets client #\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "\n",
    "    # sets cloud volume #\n",
    "    cv = cloudvolume.CloudVolume(\"graphene://https://prod.flywire-daf.com/segmentation/1.0/fly_v31\", use_https=True)\n",
    "\n",
    "    # determines resolution of volume #\n",
    "    res = cv.resolution\n",
    "\n",
    "    # converts coord strings to ints #\n",
    "    coords = [int(i) for i in coords]\n",
    "\n",
    "    # converts coordinates using volume resolution #\n",
    "    cv_xyz = [int(coords[0]/(res[0]/4)),int(coords[1]/(res[1]/4)),int(coords[2]/(res[2]/40))]\n",
    "\n",
    "    # sets point by passing converted coordinates into 'download_point' method #\n",
    "    point = int(cv.download_point(cv_xyz, size=1))\n",
    "\n",
    "    # looks up root id for that supervoxel using chunkedgraph #\n",
    "    root_result = str(client.chunkedgraph.get_root_id(supervoxel_id=point))\n",
    "\n",
    "    return root_result\n",
    "\n",
    "# defines function for querying nucleus table using list of root or nuc ids #\n",
    "# and query type ('nuc' or 'root') #\n",
    "def getNucs(ids, query_type):\n",
    "    \n",
    "    # sets client #\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "    \n",
    "    # gets current materialization version #\n",
    "    mat_vers = max(client.materialize.get_versions())\n",
    "    \n",
    "    # pulls nucleus table results based on query type #\n",
    "    if query_type == 'nuc':\n",
    "        nuc_df = client.materialize.query_table('nuclei_v1',\n",
    "                                                filter_in_dict={\"id\": ids},\n",
    "                                                materialization_version = mat_vers)\n",
    "    elif query_type == 'root':\n",
    "        nuc_df = client.materialize.query_table('nuclei_v1',\n",
    "                                            filter_in_dict={\"pt_root_id\": ids},\n",
    "                                            materialization_version = mat_vers)\n",
    "\n",
    "    # converts nucleus coordinates from n to 4x4x40 resolution #    \n",
    "    nuc_df['pt_position'] = [coordConvert(i) for i in nuc_df['pt_position']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # creates output dataframe using root id, nuc id, and nuc coords from table to keep alignment #\n",
    "    out_df = pd.DataFrame({'Root ID':list(nuc_df['pt_root_id'].astype('str')), # converts int ids to str #\n",
    "                           'Nucleus ID':list(nuc_df['id']),\n",
    "                           'Nucleus Coordinates':list(nuc_df['pt_position'])})\n",
    "        \n",
    "    return out_df.astype(str)\n",
    "\n",
    "# defines function to get presynaptic table entry using root id #\n",
    "def getSyns(root_ids, cleft_thresh=0):\n",
    "    \n",
    "    # sets client #\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "    \n",
    "    # gets current materialization version #\n",
    "    mat_vers = max(client.materialize.get_versions())\n",
    "    \n",
    "    # creates blank output dataframe #\n",
    "    out_df = pd.DataFrame(columns = ['Root ID','Incoming Synapses','Outgoing Synapses',\n",
    "                                     'Upstream Parters','Downstream Partners'])\n",
    "    \n",
    "    \n",
    "    # iterates through root ids, creating df rows and adding them to output df #\n",
    "    for i in root_ids:\n",
    "    \n",
    "        # gets pre and post synapse tables using root id #\n",
    "        pre_syn_df = client.materialize.query_table('synapses_nt_v1', \n",
    "                                                filter_in_dict={\"pre_pt_root_id\":[i]})\n",
    "        post_syn_df = client.materialize.query_table('synapses_nt_v1', \n",
    "                                                filter_in_dict={\"post_pt_root_id\":[i]})\n",
    "\n",
    "        # removes false positives by filtering out synapses with cleft scores under 50 #\n",
    "        pre_syn_df = pre_syn_df.loc[pre_syn_df['cleft_score'] >= cleft_thresh].reset_index(drop = True)\n",
    "        post_syn_df = post_syn_df.loc[post_syn_df['cleft_score'] >= cleft_thresh].reset_index(drop = True)\n",
    "\n",
    "        # counts total pre- and post-synapses #\n",
    "        pre_num = len(pre_syn_df)\n",
    "        post_num = len(post_syn_df)\n",
    "\n",
    "        # gets lists of pre- and post-synaptic partners #\n",
    "        downstream_num = len(pre_syn_df['post_pt_root_id'].unique())\n",
    "        upstream_num = len(post_syn_df['pre_pt_root_id'].unique())\n",
    "\n",
    "        # removes any false positives by filtering out non-'t' in valid_nt column #\n",
    "        pre_syn_df = pre_syn_df.loc[pre_syn_df['valid_nt'] == 't'].reset_index(drop = True)\n",
    "        post_syn_df = post_syn_df.loc[post_syn_df['valid_nt'] == 't'].reset_index(drop = True)\n",
    "\n",
    "        # calculates neurotransmitter averages #\n",
    "        pre_gaba_mean = round(pre_syn_df['gaba'].mean(),3)\n",
    "        pre_ach_mean = round(pre_syn_df['ach'].mean(),3)\n",
    "        pre_glut_mean = round(pre_syn_df['glut'].mean(),3)\n",
    "        pre_oct_mean = round(pre_syn_df['oct'].mean(),3)\n",
    "        pre_ser_mean = round(pre_syn_df['ser'].mean(),3)\n",
    "        pre_da_mean = round(pre_syn_df['da'].mean(),3)\n",
    "        post_gaba_mean = round(post_syn_df['gaba'].mean(),3)\n",
    "        post_ach_mean = round(post_syn_df['ach'].mean(),3)\n",
    "        post_glut_mean = round(post_syn_df['glut'].mean(),3)\n",
    "        post_oct_mean = round(post_syn_df['oct'].mean(),3)\n",
    "        post_ser_mean = round(post_syn_df['ser'].mean(),3)\n",
    "        post_da_mean = round(post_syn_df['da'].mean(),3)\n",
    "        \n",
    "        # makes row dataframe #\n",
    "        row_df = pd.DataFrame({'Root ID':[i], 'Incoming Synapses':[post_num], 'Outgoing Synapses':[pre_num],\n",
    "        'Upstream Parters':[upstream_num], 'Downstream Partners':[downstream_num],\n",
    "        'Post Gaba Avg':[post_gaba_mean], 'Post Ach Avg':[post_ach_mean], 'Post Glut Avg':[post_glut_mean],\n",
    "        'Post Oct Avg':[post_oct_mean], 'Post Ser Avg':[post_ser_mean], 'Post Da Avg':[post_da_mean],\n",
    "        'Pre Gaba Avg':[pre_gaba_mean], 'Pre Ach Avg':[pre_ach_mean], 'Pre Glut Avg':[pre_glut_mean],\n",
    "        'Pre Oct Avg':[pre_oct_mean], 'Pre Ser Avg':[pre_ser_mean], 'Pre Da Avg':[pre_da_mean],})\n",
    "        \n",
    "        # appends row onto output df #\n",
    "        out_df = out_df.append(row_df, ignore_index = True)\n",
    "    \n",
    "    return out_df.astype(str)\n",
    "\n",
    "# defines function to query changelog for edit data #\n",
    "def getEdits(root_ids):\n",
    "    \n",
    "    # sets client #\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "    \n",
    "    # creates blank output dataframe #\n",
    "    out_df = pd.DataFrame()\n",
    "    \n",
    "    # iterates through root ids to fill output df #\n",
    "    for i in root_ids:\n",
    "        \n",
    "        # attempts to get changelog and assign edit values using root id #\n",
    "        try:\n",
    "\n",
    "            # gets changelog dictionary using root id #\n",
    "            change_dict = client.chunkedgraph.get_change_log(i)\n",
    "\n",
    "            # sets edit data objects #\n",
    "            splits = change_dict['n_splits']\n",
    "            merges = change_dict['n_mergers']\n",
    "            edits = splits + merges\n",
    "        \n",
    "        # handles exception if query returns error #\n",
    "        except:\n",
    "            splits = 'n/a'\n",
    "            merges = 'n/a'\n",
    "            edits = 'n/a'\n",
    "        \n",
    "        # creates row df from edit data objects #\n",
    "        row_df = pd.DataFrame({'Root ID': [i],'Splits':[splits],'Merges':[merges],\n",
    "                               'Total Edits':[edits]})\n",
    "        \n",
    "        # appends row onto output df #\n",
    "        out_df = out_df.append(row_df)\n",
    "\n",
    "    return out_df.astype(str) \n",
    "\n",
    "# defines function to query proofreading table using list of root ids #\n",
    "def getProof(root_list):\n",
    "\n",
    "    # sets client #\n",
    "    client = CAVEclient('flywire_fafb_production')\n",
    "\n",
    "    # queries proofreading table using root ids #\n",
    "    proof_df = client.materialize.query_table('proofreading_status_public_v1', \n",
    "                                                filter_in_dict={'pt_root_id':root_list})\n",
    "\n",
    "\n",
    "\n",
    "    # creates balnk lists for proofreading status and proofreader id #\n",
    "    proofed_list = []\n",
    "    proofreader_list = []\n",
    "\n",
    "    # iterates over root ids and attempts to fill in proofreading status and proofreader id #\n",
    "    for i in root_list:\n",
    "        try:\n",
    "            proofed_list.append(proof_df.set_index('pt_root_id').loc[int(i),'proofread'] == 't')\n",
    "        except:\n",
    "            proofed_list.append(False)\n",
    "        try:\n",
    "            proofreader_list.append(proof_df.set_index('pt_root_id').loc[int(i),'user_id'])\n",
    "        except:\n",
    "            proofreader_list.append('n/a')\n",
    "\n",
    "    #creates output df using lists # \n",
    "    out_df = pd.DataFrame({'Root ID':root_list,\n",
    "                            'Proofread':proofed_list,\n",
    "                            'Proofreader':proofreader_list})\n",
    "\n",
    "    return out_df  \n",
    "\n",
    "# defines function to build dataframe using root id list #\n",
    "# and options (list of keywords based on checkbox input) #\n",
    "def dfBuilder(id_list, options, cleft_thresh):\n",
    "\n",
    "    # sets default query type to root id #\n",
    "    query_type = 'root'\n",
    "\n",
    "    # automatically determines data type based on length of input ids #\n",
    "    if all([len(str(i)) == 18 for i in id_list]):\n",
    "\n",
    "        # sets output df to root id list #\n",
    "        out_df = pd.DataFrame({'Root ID':id_list})\n",
    "\n",
    "        # creates nuc dataframe using getNucs #\n",
    "        nuc_df = getNucs(id_list,query_type)\n",
    "\n",
    "        # joins nuc_df to out_df #\n",
    "        out_df = out_df.join(nuc_df.set_index('Root ID'), on='Root ID')\n",
    "\n",
    "        # sets root list equal to id list #\n",
    "        root_list = id_list\n",
    "\n",
    "    elif all([len(str(i)) == 7 for i in id_list]):\n",
    "        \n",
    "        # changes query type to nucleus ids #\n",
    "        query_type = 'nuc'\n",
    "\n",
    "        # creates output dataframe using getNucs, since all ids will have associated nuclei #\n",
    "        out_df = getNucs(id_list,query_type)\n",
    "\n",
    "        # sets root list using id column of nuc df #\n",
    "        root_list = out_df['Root ID']\n",
    "\n",
    "    elif len(id_list) % 3 == 0:\n",
    "        \n",
    "        # sets root list by converting coords to root id #\n",
    "        root_list = [coordsToRoot(id_list)]\n",
    "\n",
    "        # sets output df to root id #\n",
    "        out_df = pd.DataFrame({'Root ID':root_list})\n",
    "\n",
    "        # creates nuc dataframe using getNucs #\n",
    "        nuc_df = getNucs(root_list,query_type)\n",
    "\n",
    "        # joins nuc_df to out_df #\n",
    "        out_df = out_df.join(nuc_df.set_index('Root ID'), on='Root ID')\n",
    "\n",
    "    out_df.insert(1,\"Root Current\", client.chunkedgraph.is_latest_roots(root_list))\n",
    "    \n",
    "    # adds proofreading dat if checkbox is marked #\n",
    "    if 'proof' in options:\n",
    "        \n",
    "        #creates proof_df using root list #\n",
    "        proof_df = getProof(root_list)\n",
    "\n",
    "        # joins proof_df to out_df #\n",
    "        out_df = out_df.join(proof_df.set_index('Root ID'), on='Root ID')\n",
    "\n",
    "    # adds edit data if edits checkbox is marked CHANGE THIS TO TABULAR LOG #\n",
    "    if 'edits' in options: \n",
    "        \n",
    "        # creates edit_df using root list #\n",
    "        edit_df = getEdits(root_list)\n",
    "        \n",
    "        # joins edit_df to out_df #\n",
    "        out_df = out_df.join(edit_df.set_index('Root ID'), on='Root ID')\n",
    "\n",
    "        # adds synapse data if synapse checkbox is marked #\n",
    "    if 'syns' in options:\n",
    "        \n",
    "        # creates syn_df by passing root list into getSyns #\n",
    "        syn_df = getSyns(root_list, cleft_thresh)\n",
    "        \n",
    "        # joins syn_df to out_df #\n",
    "        out_df = out_df.join(syn_df.set_index('Root ID'), on='Root ID')\n",
    "\n",
    "          \n",
    "    return out_df.astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASH APP #\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# defines layout of various app elements (submission field, checkboxes, button, output table) #\n",
    "app.layout = html.Div([\n",
    "    # defines text area to relay messages #\n",
    "    dcc.Textarea(\n",
    "        id='message_text',\n",
    "        value='Input coordinates, select output parameters, and click \"Submit\" button.\\n'\\\n",
    "        'ID queries are limited to 20 entries, coordinate lookups must be done one at a time.\\n'\\\n",
    "        'Lookup takes ~2-3 seconds per entry.',\n",
    "        style={'width': '650px','resize': 'none'},\n",
    "        rows=3,\n",
    "        disabled=True,\n",
    "    ),\n",
    "    # defines input field for ids #\n",
    "    html.Div(dcc.Input(  \n",
    "        id='input_field', \n",
    "        type='text', \n",
    "        placeholder='ID Number',\n",
    "    )),\n",
    "    html.Br(\n",
    "    ),\n",
    "    # defines data selection checkboxes #\n",
    "    html.Div(dcc.Checklist(  \n",
    "        id='checkboxes',\n",
    "        options=[\n",
    "            {'label': 'Proofreading Status and Proofreader ID', 'value': 'proof'},\n",
    "            {'label': 'Splits, Mergers, and Total Edits', 'value': 'edits'},\n",
    "            {'label': 'Synapse Counts and Neurotransmitters', 'value': 'syns'},\n",
    "        ],\n",
    "        labelStyle={'display': 'block'},\n",
    "        value=['syns','edits','proof'],\n",
    "    )),\n",
    "    # defines message explaining cleft score field #\n",
    "    dcc.Textarea(\n",
    "        id='cleft_message_text',\n",
    "        value='Cleft score threshold for synapses (default is 50)',\n",
    "        style={'width': '400px','resize': 'none'},\n",
    "        rows=1,\n",
    "        disabled=True,\n",
    "    ),\n",
    "    # defines input field for cleft score threshold #\n",
    "    html.Div(dcc.Input(  \n",
    "        id='cleft_thresh_field', \n",
    "        type='number',\n",
    "        value=50,\n",
    "        \n",
    "    )),\n",
    "    html.Br(\n",
    "    ),\n",
    "    # defines submission button #\n",
    "    html.Button(  \n",
    "        'Submit', \n",
    "        id='submit_button', \n",
    "        n_clicks=0,\n",
    "    ),\n",
    "    html.Br(\n",
    "    ),\n",
    "    # defines output table #\n",
    "    html.Div(dash_table.DataTable(  \n",
    "        id='table', \n",
    "        fill_width=False,  # sets column width to fit text instead of expanding to container width #\n",
    "        export_format=\"csv\",\n",
    "    ))\n",
    "])\n",
    "\n",
    "# defines callback that takes root ids and desired data selection on button click and generates table #\n",
    "@app.callback(\n",
    "    Output('table','columns'),           # defines first output location as 'columns' aspect of 'table' #\n",
    "    Output('table', 'data'),             # defines second output location as 'data' aspect of 'table' #\n",
    "    Output('message_text','value'),      # defines second output location as 'data' aspect of 'table' #\n",
    "    Input('submit_button', 'n_clicks'),  # defines trigger as button press # \n",
    "    State('input_field', 'value'),       # defines first input state as value of 'input_field' #\n",
    "    State('checkboxes','value'),         # defines second input state as value of 'checkboxes' #\n",
    "    State('cleft_thresh_field','value'),  # defines third input state as value of 'cleft_thresh_field' #\n",
    "    prevent_initial_call=True,           # prevents function from executing prior to button press #\n",
    ")\n",
    "def update_output(n_clicks, ids, checked, cleft_thresh):\n",
    "    \n",
    "    # splits 'roots' string into list and strips spaces #\n",
    "    id_list = [x.strip(' ') for x in str(ids).split(\",\")] \n",
    "    \n",
    "    # if query list under 20 items, generates dataframe #\n",
    "    if len(id_list) <= 20:\n",
    "        \n",
    "        # creates df using dfBuilder function #\n",
    "        df = dfBuilder(id_list, checked, cleft_thresh)\n",
    "        \n",
    "        # converts nucleus coords to str to avoid issues when passing to Dash table #\n",
    "        df['Nucleus Coordinates'] = df['Nucleus Coordinates'].astype('str')\n",
    "        \n",
    "        # creates column list based on dataframe columns #\n",
    "        column_list = [{\"name\": i, \"id\": i} for i in df.columns]\n",
    "        \n",
    "        # converts df to dictionary to pass to Dash table #\n",
    "        df_dict =  df.to_dict('records')\n",
    "        \n",
    "        # keeps message output the same #\n",
    "        message_output = 'Input coordinates, select output parameters, and click \"Submit\" button.\\n'\\\n",
    "        'ID queries are limited to 20 entries, coordinate lookups must be done one at a time.\\n'\\\n",
    "        'Lookup takes ~2-3 seconds per entry.'\n",
    "        \n",
    "        # builds output list #\n",
    "        output_list = [column_list,df_dict,message_output]        #combines df_dict and column_list into output list#\n",
    "        \n",
    "        return output_list                                        #returns list of column names and data values#\n",
    "    \n",
    "    # returns error message if query list is longer than 20 items #\n",
    "    else:\n",
    "        return [0,0,'Please limit each query to a maximum of 20 items.']\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
