{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "695b63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATIONS#\n",
    "\n",
    "# core importations\n",
    "import caveclient\n",
    "from caveclient import CAVEclient\n",
    "import cloudvolume\n",
    "from cloudvolume import CloudVolume\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dash importations\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output, State, dash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0f4f1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS#\n",
    "\n",
    "# defines function to convert nm coordinates into FlyWire-usable 4,4,40 #\n",
    "def coordConvert(coords):\n",
    "    x = coords\n",
    "    x[0] /= 4\n",
    "    x[1] /= 4\n",
    "    x[2] /= 40\n",
    "    return x\n",
    "\n",
    "# defines function to convert list of x,y,z coordinates in [4,4,40] resolution to root id #\n",
    "def coordsToRoot(coords):\n",
    "    \n",
    "    # converts coordinates to ints #\n",
    "    coords = list(map(int,coords))\n",
    "\n",
    "    # sets client #\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "\n",
    "    # sets cloud volume #\n",
    "    cv = cloudvolume.CloudVolume(\"graphene://https://prod.flywire-daf.com/segmentation/1.0/fly_v31\", use_https=True)\n",
    "\n",
    "    # determines resolution of volume #\n",
    "    res = cv.resolution\n",
    "\n",
    "    # converts coordinates using volume resolution #\n",
    "    cv_xyz = [\n",
    "        int(coords[0]/(res[0]/4)),\n",
    "        int(coords[1]/(res[1]/4)),\n",
    "        int(coords[2]/(res[2]/40))\n",
    "        ]\n",
    "\n",
    "    # sets point by passing converted coordinates into 'download_point' method #\n",
    "    point = int(cv.download_point(cv_xyz, size=1))\n",
    "\n",
    "    # looks up root id for that supervoxel using chunkedgraph, converts to string #\n",
    "    root_result = str(client.chunkedgraph.get_root_id(supervoxel_id=point))\n",
    "\n",
    "    return root_result\n",
    "\n",
    "# defines function for querying nucleus table using list of root or nuc ids #\n",
    "# and query type ('nuc' or 'root') #\n",
    "def getNuc(id_list):\n",
    "    \n",
    "    # sets client #\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "    \n",
    "    # gets current materialization version #\n",
    "    mat_vers = max(client.materialize.get_versions())\n",
    "    \n",
    "    # pulls nucleus table results based on query type #\n",
    "    if len(id_list[0]) == 7:\n",
    "        nuc_df = client.materialize.query_table(\n",
    "            'nuclei_v1',\n",
    "            filter_in_dict={\"id\": id_list},\n",
    "            materialization_version = mat_vers\n",
    "            )\n",
    "    elif len(id_list[0]) == 18:\n",
    "        nuc_df = client.materialize.query_table(\n",
    "            'nuclei_v1',\n",
    "            filter_in_dict={\"pt_root_id\": id_list},\n",
    "            materialization_version = mat_vers\n",
    "            )\n",
    "\n",
    "    # converts nucleus coordinates from n to 4x4x40 resolution #    \n",
    "    nuc_df['pt_position'] = [coordConvert(i) for i in nuc_df['pt_position']]\n",
    "    \n",
    "    # creates output dataframe using root id, nuc id, and nuc coords from table to keep alignment #\n",
    "    out_df = pd.DataFrame({\n",
    "        'Root ID':list(nuc_df['pt_root_id']),\n",
    "        'Nucleus ID':list(nuc_df['id']),\n",
    "        'Nucleus Coordinates':list(nuc_df['pt_position'])\n",
    "        })\n",
    "        \n",
    "    return out_df.astype(str)\n",
    "\n",
    "# defines function to create df of nt averages by passing: #\n",
    "# list of partner ids, pre- or post-synapse df, and matching column name #\n",
    "def ntMeans(ids,df,col_name):\n",
    "\n",
    "    # makes blank output dataframe #\n",
    "    out_df = pd.DataFrame()\n",
    "    \n",
    "    # iterates through partner ids #\n",
    "    for x in ids:\n",
    "\n",
    "        #!!! MAY BE PROBLEM !!!#\n",
    "        # filters main df to only include entries for partner #\n",
    "        partner_df = (df.loc[df[col_name] == x]).reset_index(drop = True)\n",
    "\n",
    "        # creates row dataframe and fills with nt avgs#\n",
    "        row_df = pd.DataFrame({'Partner ID':[x]})\n",
    "        row_df['Gaba Avg'] = [round(partner_df['gaba'].mean(),3)]\n",
    "        row_df['Ach Avg'] = [round(partner_df['ach'].mean(),3)]\n",
    "        row_df['Glut Avg'] = [round(partner_df['glut'].mean(),3)]\n",
    "        row_df['Oct Avg'] = [round(partner_df['oct'].mean(),3)]\n",
    "        row_df['Ser Avg'] = [round(partner_df['ser'].mean(),3)]\n",
    "        row_df['Da Avg'] = [round(partner_df['da'].mean(),3)]\n",
    "\n",
    "        # adds row df to output df #\n",
    "        out_df = out_df.append(row_df).reset_index(drop = True)\n",
    "\n",
    "    return out_df\n",
    "\n",
    "# defines function to validate constructed synapse data #\n",
    "def checkValid(up_df, down_df, incoming_df, outgoing_df):\n",
    "\n",
    "    # sets counter #\n",
    "    counter = 0\n",
    "\n",
    "    # validates upstream partners #\n",
    "    for x in up_df.index:\n",
    "        built_con = up_df.loc[x,'Connections']\n",
    "        quer_con = str(\n",
    "            list(incoming_df['pre_pt_root_id'].astype(str)).count(\n",
    "                up_df.loc[x,'Upstream Partner ID']))\n",
    "        \n",
    "        if(\n",
    "            built_con == quer_con\n",
    "            # round(np.mean(out_df['gaba']),3) == row_df.loc[0,'Gaba Avg']\n",
    "            # round(np.mean(out_df['ach']),3) == row_df.loc[0,'Ach Avg'] and\n",
    "            # round(np.mean(out_df['glut']),3) == row_df.loc[0,'Glut Avg'] and\n",
    "            # round(np.mean(out_df['oct']),3) == row_df.loc[0,'Oct Avg'] and\n",
    "            # round(np.mean(out_df['ser']),3) == row_df.loc[0,'Ser Avg'] and\n",
    "            # round(np.mean(out_df['da']),3) == row_df.loc[0,'Da Avg']\n",
    "        ):\n",
    "            counter += 1\n",
    "        else:\n",
    "            failed = counter + ' items validated. Upstream data false for partner '+ up_df.loc[x,'Upstream Partner ID'] + \\\n",
    "                    '. Built count = ' + built_con + '. Query count = ' + quer_con\n",
    "            return failed\n",
    "    \n",
    "    # validates downstream partners #\n",
    "    for x in down_df.index:\n",
    "        built_con = down_df.loc[x,'Connections']\n",
    "        quer_con = str(\n",
    "            list(outgoing_df['post_pt_root_id'].astype(str)).count(\n",
    "                down_df.loc[x,'Downstream Partner ID']))\n",
    "        \n",
    "        if(\n",
    "            built_con == quer_con\n",
    "        ):\n",
    "            counter += 1\n",
    "        else:\n",
    "            failed = counter + ' items validated. Downstream data false for partner '+ down_df.loc[x,'Downstream Partner ID'] + \\\n",
    "                    '. Built count = ' + built_con + '. Query count = ' + quer_con\n",
    "            return failed\n",
    "    \n",
    "    return 'All ' + str(counter) + ' items have been validated.'\n",
    "\n",
    "# defines function to get synapse info using root ID#\n",
    "def getSyn(root_id, cleft_thresh=0, validate=False):\n",
    "\n",
    "    #sets client#\n",
    "    client = CAVEclient(\"flywire_fafb_production\")\n",
    "    \n",
    "    #gets current materialization version#\n",
    "    mat_vers = max(client.materialize.get_versions())\n",
    "    \n",
    "    #makes dfs of pre- (outgoing) and post- (incoming) synapses #\n",
    "    outgoing_syn_df = client.materialize.query_table(\n",
    "        'synapses_nt_v1',\n",
    "        filter_in_dict={\"pre_pt_root_id\":root_id},\n",
    "        materialization_version = mat_vers\n",
    "        )\n",
    "    incoming_syn_df = client.materialize.query_table(\n",
    "        'synapses_nt_v1',\n",
    "        filter_in_dict={\"post_pt_root_id\":root_id},\n",
    "        materialization_version = mat_vers\n",
    "        )\n",
    "\n",
    "    # removes synapses below cleft threshold, 0-roots, and autapses #\n",
    "    outgoing_syn_df = outgoing_syn_df[outgoing_syn_df['cleft_score'] >= cleft_thresh].reset_index(drop = True)\n",
    "    outgoing_syn_df = outgoing_syn_df[outgoing_syn_df[\"pre_pt_root_id\"] != outgoing_syn_df[\"post_pt_root_id\"]].reset_index(drop = True)\n",
    "    outgoing_syn_df = outgoing_syn_df[outgoing_syn_df[\"post_pt_root_id\"] != 0].reset_index(drop = True)\n",
    "    incoming_syn_df = incoming_syn_df[incoming_syn_df['cleft_score'] >= cleft_thresh].reset_index(drop = True)\n",
    "    incoming_syn_df = incoming_syn_df[incoming_syn_df[\"pre_pt_root_id\"] != incoming_syn_df[\"post_pt_root_id\"]].reset_index(drop = True)\n",
    "    incoming_syn_df = incoming_syn_df[incoming_syn_df[\"post_pt_root_id\"] != 0].reset_index(drop = True)\n",
    "\n",
    "    # calculates total synapses #\n",
    "    in_count = len(incoming_syn_df)\n",
    "    out_count = len(outgoing_syn_df)\n",
    "    \n",
    "    # gets lists of pre and post synaptic partners #\n",
    "    downstream_partners = list(outgoing_syn_df.drop_duplicates(subset = 'post_pt_root_id')['post_pt_root_id'])\n",
    "    upstream_partners = list(incoming_syn_df.drop_duplicates(subset = 'pre_pt_root_id')['pre_pt_root_id'])\n",
    "\n",
    "    # calculates number of upstream and downstream partners #\n",
    "    up_count = len(upstream_partners)\n",
    "    down_count = len(downstream_partners)\n",
    "\n",
    "    # builds output dataframes #\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Root ID':root_id,\n",
    "        'Incoming':in_count,\n",
    "        'Outgoing':out_count,\n",
    "        'Upstream Partners':up_count,\n",
    "        'Downstream Partners':down_count\n",
    "        })\n",
    "    up_df = pd.DataFrame({'Partner ID':upstream_partners})\n",
    "    down_df = pd.DataFrame({'Partner ID':downstream_partners})\n",
    "\n",
    "    # adds number of connections between input neuron and partners #\n",
    "    up_df['Connections'] = [list(incoming_syn_df['pre_pt_root_id']).count(x) for x in upstream_partners]\n",
    "    down_df['Connections'] = [list(outgoing_syn_df['post_pt_root_id']).count(x) for x in downstream_partners]\n",
    "\n",
    "    # adds neurotransmitter averages for each partner #\n",
    "    up_df = up_df.join(\n",
    "        ntMeans(\n",
    "            upstream_partners,\n",
    "            incoming_syn_df,\n",
    "            'pre_pt_root_id'\n",
    "            ).set_index('Partner ID'), \n",
    "        on='Partner ID'\n",
    "        )\n",
    "    down_df = down_df.join(\n",
    "        ntMeans(\n",
    "            downstream_partners,\n",
    "            outgoing_syn_df,\n",
    "            'post_pt_root_id'\n",
    "            ).set_index('Partner ID'), \n",
    "        on='Partner ID'\n",
    "        )\n",
    "\n",
    "    # renames partner id columns to up/downstream #\n",
    "    up_df = up_df.rename(columns={\"Partner ID\": \"Upstream Partner ID\"})\n",
    "    down_df = down_df.rename(columns={\"Partner ID\": \"Downstream Partner ID\"})\n",
    "\n",
    "    # converts all data to strings #\n",
    "    summary_df = summary_df.astype(str)\n",
    "    up_df = up_df.astype(str)\n",
    "    down_df = down_df.astype(str)\n",
    "    \n",
    "    # runs data validation if input variable is set to True #\n",
    "    if validate == True:\n",
    "        val_out = checkValid(up_df, down_df, incoming_syn_df, outgoing_syn_df)\n",
    "        return [summary_df,up_df,down_df, val_out]\n",
    "    else:\n",
    "        return [summary_df,up_df,down_df,'Data not validated']\n",
    "\n",
    "# defines function to build dataframe using list-formatted root/nuc id or coords #\n",
    "def dfBuilder(input_list, cleft_thresh, validate):\n",
    "\n",
    "    # if coordinates detected, converts to root #\n",
    "    if len(input_list) == 3:\n",
    "        input_list = [coordsToRoot(input_list)]\n",
    "\n",
    "    # uses root or nuc id to build nuc df #\n",
    "    nuc_df = getNuc(input_list)\n",
    "\n",
    "    # uses root id to build synapse dataframes #\n",
    "    syn_sum_df, up_df, down_df, val_status = getSyn(\n",
    "        [str(nuc_df.loc[0,'Root ID'])], \n",
    "        cleft_thresh,\n",
    "        validate,\n",
    "        )\n",
    "\n",
    "    # joins synapse summary to nucleus df to create summary df\n",
    "    sum_df = nuc_df.join(\n",
    "        syn_sum_df.set_index('Root ID'), \n",
    "        on='Root ID'\n",
    "        )\n",
    "    \n",
    "    #returns output dataframes#\n",
    "    return [sum_df, up_df, down_df, val_status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "83e42858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = CAVEclient(\"flywire_fafb_production\")\n",
    "# mat_vers = max(client.materialize.get_versions())\n",
    "# test_df = client.materialize.query_table(\n",
    "#     'synapses_nt_v1',\n",
    "#     filter_in_dict={\n",
    "#         \"pre_pt_root_id\":[720575940628522967],\n",
    "#         # \"post_pt_root_id\":[post_root_id],\n",
    "#         },\n",
    "#     materialization_version = mat_vers\n",
    "# )\n",
    "\n",
    "# test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9b7f65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:00] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:00] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:00] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:00] \"GET /_favicon.ico?v=2.0.0 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:00] \"GET /_dash-component-suites/dash/dash_table/async-highlight.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:00] \"GET /_dash-component-suites/dash/dash_table/async-table.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Nov/2021 15:03:05] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# DASH APP #\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# defines layout of various app elements (submission field, checkboxes, button, output table)#\n",
    "app.layout = html.Div([\n",
    "    \n",
    "    #defines text area for instructions and feedback#\n",
    "    dcc.Textarea(\n",
    "        id='message_text',\n",
    "        value='Input root/nuc ID or coordinates and click \"Submit\" button.\\n'\\\n",
    "            'Only one entry at a time.',\n",
    "        style={'width': '500px','resize': 'none'},\n",
    "        rows=2,\n",
    "        disabled=True,\n",
    "    ),\n",
    "    \n",
    "    #defines input field#\n",
    "    html.Div(dcc.Input(  \n",
    "        id='input_field', \n",
    "        type='text', \n",
    "        placeholder='Root/Nuc ID or Coordinates',\n",
    "    )),\n",
    "    \n",
    "    html.Br(\n",
    "    ),\n",
    "\n",
    "    # defines message explaining cleft score field #\n",
    "    dcc.Textarea(\n",
    "        id='cleft_message_text',\n",
    "        value='Cleft score threshold for synapses:',\n",
    "        style={'width': '400px','resize': 'none'},\n",
    "        rows=1,\n",
    "        disabled=True,\n",
    "    ),\n",
    "\n",
    "    # defines input field for cleft score threshold #\n",
    "    html.Div(dcc.Input(  \n",
    "        id='cleft_thresh_field', \n",
    "        type='number',\n",
    "        value=50,\n",
    "        \n",
    "    )),\n",
    "\n",
    "    html.Br(\n",
    "    ),\n",
    "\n",
    "    # defines validation checkbox #\n",
    "    html.Div(dcc.Checklist(  \n",
    "        id='val_check',\n",
    "        options=[{'label': 'Data Validation','value': True}],\n",
    "        labelStyle={'display': 'block'},\n",
    "    )),\n",
    "\n",
    "    #defines submission button#\n",
    "    html.Button(  \n",
    "        'Submit', \n",
    "        id='submit_button', \n",
    "        n_clicks=0,\n",
    "        style={'margin-top': '15px','margin-bottom': '15px'}\n",
    "    ),\n",
    "    \n",
    "    html.Br(\n",
    "    ),\n",
    "    \n",
    "    #defines summary table#\n",
    "    html.Div(dash_table.DataTable(  \n",
    "        id='summary_table', \n",
    "        fill_width=False, #sets column width to fit text instead of expanding to container width# \n",
    "        export_format=\"csv\",\n",
    "    )),\n",
    "\n",
    "    html.Br(\n",
    "    ),\n",
    "\n",
    "    #defines incoming table#\n",
    "    html.Div(dash_table.DataTable(  \n",
    "        id='incoming_table', \n",
    "        # sets column width to fit text instead of expanding to container width #\n",
    "        # fill_width=False, \n",
    "        export_format=\"csv\",\n",
    "        style_table={'height': '180px', 'overflowY': 'auto'},\n",
    "        page_action='none',\n",
    "        fixed_rows={'headers': True},\n",
    "        style_cell={'width': 160},\n",
    "    )),\n",
    "\n",
    "    html.Br(\n",
    "    ),\n",
    "    \n",
    "    #defines outgoing table#\n",
    "    html.Div(dash_table.DataTable(  \n",
    "        id='outgoing_table', \n",
    "        # sets column width to fit text instead of expanding to container width #\n",
    "        # fill_width=False,  \n",
    "        export_format=\"csv\",\n",
    "        style_table={'height': '180px', 'overflowY': 'auto'},\n",
    "        page_action='none',\n",
    "        fixed_rows={'headers': True},\n",
    "        style_cell={'width': 160},\n",
    "    ))\n",
    "])\n",
    "\n",
    "#defines callback that takes root ids and desired data selection on button click and generates table#\n",
    "@app.callback(\n",
    "    Output('summary_table','columns'),\n",
    "    Output('summary_table', 'data'),\n",
    "    Output('incoming_table','columns'),\n",
    "    Output('incoming_table', 'data'),\n",
    "    Output('outgoing_table','columns'),\n",
    "    Output('outgoing_table', 'data'),\n",
    "    Output('message_text','value'),\n",
    "    Input('submit_button', 'n_clicks'),  #defines trigger as button press (change in the state of the 'n_clicks' aspect of 'submit_button')# \n",
    "    State('input_field', 'value'),\n",
    "    State('cleft_thresh_field','value'),\n",
    "    State('val_check','value'),\n",
    "    prevent_initial_call=True,           #prevents function from being called on page load (prior to input)#\n",
    ")\n",
    "def update_output(n_clicks, input_list, cleft_thresh,val_choice):\n",
    "\n",
    "    # splits 'ids' string into list #\n",
    "    input_list = str(input_list).split(\",\")\n",
    "    \n",
    "    # strips spaces from id_list entries and converts to integers #\n",
    "    input_list = [str(x.strip(' ')) for x in input_list]\n",
    "\n",
    "    # builds output if 1-item threshold isn't exceeded #\n",
    "    if len(input_list) == 1 or len(input_list) == 3 and len(input_list[0]) != len(input_list[2]):\n",
    "\n",
    "        if val_choice == True:\n",
    "            val_in = True\n",
    "        else:\n",
    "            val_in = False\n",
    "\n",
    "        # sets dataframes by passing id/coords into dfBuilder function #\n",
    "        sum_df, up_df, down_df , val_status = dfBuilder(input_list, cleft_thresh, val_in)\n",
    "\n",
    "        # creates column lists based on dataframe columns #\n",
    "        sum_column_list = [{\"name\": i, \"id\": i} for i in sum_df.columns]\n",
    "        up_column_list = [{\"name\": i, \"id\": i} for i in up_df.columns]\n",
    "        down_column_list = [{\"name\": i, \"id\": i} for i in down_df.columns]\n",
    "        \n",
    "        # makes dictionaries from dataframes #\n",
    "        sum_dict =  sum_df.to_dict('records')\n",
    "        up_dict =  up_df.to_dict('records')\n",
    "        down_dict =  down_df.to_dict('records')\n",
    "\n",
    "        # changes message to reflect validation status #\n",
    "        message_output = val_status\n",
    "        \n",
    "        #returns list of column names, data values, and message text#\n",
    "        return [\n",
    "            sum_column_list, \n",
    "            sum_dict, \n",
    "            up_column_list, \n",
    "            up_dict, \n",
    "            down_column_list, \n",
    "            down_dict, \n",
    "            message_output\n",
    "        ]               \n",
    "    \n",
    "    # returns error message if 1-item threshold is exceeded #\n",
    "    else:\n",
    "        return [0,0,0,0,0,0,'Please limit each query to one entry.']\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
